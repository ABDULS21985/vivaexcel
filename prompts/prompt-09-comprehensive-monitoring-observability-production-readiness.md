# PROMPT 9: Comprehensive Monitoring, Observability & Production Readiness

You are building a full-stack monitoring, alerting, and incident management system for the KTBlog digital product marketplace monorepo at `/Users/mac/codes/vivaexcel`. The tech stack is NestJS 11 backend (TypeORM, PostgreSQL via `DataSource`, Redis via `apps/backend/src/shared/redis/redis.service.ts` with an `isHealthy()` method, Stripe, Socket.io, `@nestjs/schedule` for CRON, `prom-client` for Prometheus via the global `MetricsModule` at `apps/backend/src/metrics/`) and Next.js 16 frontend (React 19, Tailwind CSS 4, Framer Motion, TanStack React Query, Recharts, Lucide icons, next-intl). The existing `MetricsService` at `apps/backend/src/metrics/metrics.service.ts` already defines these Prometheus metrics on a dedicated registry: `http_request_duration_seconds` (histogram, labels: method/route/status_code), `http_requests_total` (counter), `http_active_connections` (gauge), `database_query_duration_seconds` (histogram, labels: operation/table), `database_connection_pool_size` (gauge, labels: state), `user_registrations_total`, `user_logins_total`, `api_errors_total` (counter, labels: type/route), `cache_hits_total`, `cache_misses_total`. The `MetricsController` exposes `GET /metrics` returning Prometheus text format. The `HealthModule` at `apps/backend/src/health/` uses `@nestjs/terminus` with `HealthController` exposing `GET /health` (memory only) and `GET /health/ready` (database + Redis + memory) via `DatabaseHealthIndicator` (runs `SELECT 1`), `RedisHealthIndicator` (calls `redisService.isHealthy()`), `MemoryHealthIndicator`. The `CorrelationIdMiddleware` at `apps/backend/src/common/middleware/correlation-id.middleware.ts` generates nanoid correlation IDs, attaches to `req.correlationId` and `x-correlation-id` header. The `LoggingInterceptor` at `apps/backend/src/common/interceptors/logging.interceptor.ts` logs request/response with timing, optionally records metrics via injected `IMetricsService` interface. Pino logging is configured at `apps/backend/src/logging/pino.config.ts` with JSON output in production, pino-pretty in dev, custom serializers, redacted fields, correlation ID propagation, custom log levels by status code. The `AppModule` at `apps/backend/src/app.module.ts` applies `CorrelationIdMiddleware` to all routes. Sentry is referenced in backend config but not fully integrated. The frontend seller dashboard is at `apps/frontend/app/[locale]/seller-dashboard/` and the admin dashboard at `apps/dashboard/`. All entities extend `BaseEntity` (uuid PK, `createdAt`, `updatedAt`, `deletedAt`). Controllers return `{ status: 'success', message: string, data: T }` and use `JwtAuthGuard`/`RolesGuard` with `Role.ADMIN`/`Role.SUPER_ADMIN`.

## Implementation Steps

### Step 1 -- Create Entity Files for Monitoring System

Create these entity files in `apps/backend/src/entities/`:

**1a. `service-status.entity.ts`** -- Entity `service_statuses` extending `BaseEntity`. Columns: `serviceName` (varchar, unique, indexed -- e.g., "postgresql", "redis", "stripe-api", "s3-storage", "smtp-email", "api-server"), `displayName` (varchar -- e.g., "PostgreSQL Database"), `status` (enum `ServiceStatus`: `UP`, `DEGRADED`, `DOWN`), `latency` (int, nullable -- milliseconds of last health check), `lastCheckedAt` (timestamp), `metadata` (jsonb, nullable -- e.g., `{ connectionPoolActive: 8, connectionPoolIdle: 2, memoryUsageMB: 256 }`), `uptimePercentage` (decimal 5,2, default 100.00 -- rolling 90-day uptime). Add index on `status`.

**1b. `incident-log.entity.ts`** -- Entity `incident_logs` extending `BaseEntity`. Columns: `title` (varchar), `description` (text), `severity` (enum `IncidentSeverity`: `P1`, `P2`, `P3`, `P4`), `status` (enum `IncidentStatus`: `INVESTIGATING`, `IDENTIFIED`, `MONITORING`, `RESOLVED`), `servicesAffected` (jsonb typed as `string[]` -- array of `serviceName` values), `timeline` (jsonb typed as `Array<{ timestamp: string; update: string; author?: string }>`), `startedAt` (timestamp, default `NOW()`), `resolvedAt` (timestamp, nullable), `postmortemUrl` (varchar, nullable), `acknowledgedBy` (varchar, nullable -- admin userId who acknowledged). Add indexes on `severity`, `status`, `startedAt`.

**1c. `alert-rule.entity.ts`** -- Entity `alert_rules` extending `BaseEntity`. Columns: `name` (varchar -- e.g., "High Error Rate"), `description` (text, nullable), `metric` (varchar -- the Prometheus metric name, e.g., "api_errors_total", "http_request_duration_seconds"), `condition` (enum `AlertCondition`: `GT`, `LT`, `EQ`, `GTE`, `LTE`), `threshold` (decimal 12,4), `duration` (int -- seconds the condition must persist before alerting, e.g., 60), `evaluationInterval` (int, default 60 -- seconds between checks), `channel` (enum `AlertChannel`: `EMAIL`, `WEBHOOK`, `SLACK`), `recipients` (jsonb typed as `string[]` -- email addresses or webhook URLs), `isActive` (boolean, default true), `cooldownMinutes` (int, default 30 -- don't re-alert within this window), `lastTriggeredAt` (timestamp, nullable), `lastValue` (decimal 12,4, nullable -- the metric value when last checked), `triggerCount` (int, default 0). Add index on `isActive`.

**1d. `performance-budget.entity.ts`** -- Entity `performance_budgets` extending `BaseEntity`. Columns: `route` (varchar -- e.g., "/", "/products/:id", "/checkout"), `metricName` (enum `WebVitalMetric`: `LCP`, `FID`, `CLS`, `FCP`, `TTFB`, `INP`), `budgetValue` (decimal 8,3 -- e.g., 2.500 for 2.5s LCP), `currentP75` (decimal 8,3, nullable -- the current p75 value from recent reports), `currentP99` (decimal 8,3, nullable), `isCompliant` (boolean, default true), `sampleCount` (int, default 0), `lastReportedAt` (timestamp, nullable). Unique composite index on `[route, metricName]`.

**1e. `web-vital-report.entity.ts`** -- Entity `web_vital_reports` (no soft delete -- use TTL cleanup). Columns: `id` (uuid PK), `createdAt` (timestamp), `route` (varchar, indexed), `metricName` (enum reusing `WebVitalMetric`), `value` (decimal 8,3), `rating` (enum `WebVitalRating`: `GOOD`, `NEEDS_IMPROVEMENT`, `POOR`), `userAgent` (varchar, nullable), `country` (varchar length 2, nullable), `connectionType` (varchar, nullable -- e.g., "4g", "wifi"). Add index on `[route, metricName, createdAt]`. No `deletedAt` needed -- these are append-only metrics that get purged by CRON after 90 days.

### Step 2 -- Enhance the Health Module

**2a. Extend `apps/backend/src/health/health.controller.ts`**: Add a new endpoint `GET /health/detailed` (public) that returns granular health data for every dependency. Create new health indicators:

**2b. Create `apps/backend/src/health/indicators/stripe.indicator.ts`** -- `StripeHealthIndicator` extending `HealthIndicator`. Inject `ConfigService` to get `STRIPE_SECRET_KEY`. In `isHealthy()`, call `stripe.balance.retrieve()` (lightweight Stripe API call). Return status with `{ apiVersion, livemode }`. Catch errors and return unhealthy with error message. Cache the result in a class-level variable for 60 seconds to avoid rate-limiting.

**2c. Create `apps/backend/src/health/indicators/storage.indicator.ts`** -- `StorageHealthIndicator`. Inject the S3/B2 client (from `MediaModule`'s service). In `isHealthy()`, call `headBucket` or `listObjectsV2` with `MaxKeys: 1` on the configured bucket. Return status with bucket name and region.

**2d. Create `apps/backend/src/health/indicators/email.indicator.ts`** -- `EmailHealthIndicator`. Inject `ConfigService` for SMTP settings. In `isHealthy()`, create a transient SMTP connection using `nodemailer.createTransport()` with the configured SMTP host/port, call `.verify()` to test connectivity, then close. Return status with SMTP host.

**2e. Enhance `DatabaseHealthIndicator`** at `apps/backend/src/health/indicators/database.indicator.ts`: In addition to `SELECT 1`, also run `SELECT count(*) as active FROM pg_stat_activity WHERE state = 'active'` and `SELECT pg_database_size(current_database()) as size_bytes` to return `{ active_connections, database_size_mb }` in the health metadata.

**2f. Enhance `RedisHealthIndicator`** at `apps/backend/src/health/indicators/redis.indicator.ts`: After the ping check, also call `INFO memory` (via `redis.info('memory')`) to extract `used_memory_human` and `INFO stats` for `keyspace_hits`/`keyspace_misses` to compute hit rate. Return `{ usedMemory, hitRate, connectedClients }` in metadata.

**2g. Update `GET /health/detailed`** to call all five indicators (database, redis, stripe, storage, email) in parallel using `Promise.allSettled`. For each, record latency (time the check took). Save results to `service_statuses` table (upsert by `serviceName`). Also add the api-server itself as a `service_status` entry with status UP and latency 0 (since the endpoint itself responding proves the server is up). Return the full status map.

### Step 3 -- Extend Prometheus Metrics

**3a. Add new metrics to `apps/backend/src/metrics/metrics.service.ts`**:
- `active_websocket_connections` -- Gauge, no labels. Increment on Socket.io `connection`, decrement on `disconnect`.
- `stripe_webhook_processing_seconds` -- Histogram, labels: `event_type`. Observe in the Stripe webhook handler.
- `order_total_amount` -- Histogram, labels: `currency`. Observe on order completion, buckets `[5, 10, 25, 50, 100, 250, 500, 1000]`.
- `active_cart_count` -- Gauge. Set periodically from a count query on non-expired carts.
- `product_view_total` -- Counter, labels: `product_type`, `source`. Increment on each product view.
- `redis_operations_total` -- Counter, labels: `operation` (get/set/del), `status` (success/error). Increment in `RedisService` methods.
- `email_sent_total` -- Counter, labels: `template`, `status` (success/error). Increment in `EmailService`.
- `ai_request_duration_seconds` -- Histogram, labels: `operation` (title_suggestion/content_analysis/etc). Observe in `AiService`.

Add corresponding helper methods: `recordWebSocketConnection()`, `recordWebSocketDisconnection()`, `recordStripeWebhook(eventType, durationSeconds)`, `recordOrder(amount, currency)`, `setActiveCartCount(count)`, `recordProductView(productType, source)`, `recordRedisOperation(operation, status)`, `recordEmailSent(template, status)`, `recordAiRequest(operation, durationSeconds)`.

**3b. Create `apps/backend/src/common/interceptors/performance.interceptor.ts`** -- `PerformanceInterceptor` implementing `NestInterceptor`. Inject `MetricsService`. In `intercept()`: record start time, extract route pattern from `context.switchToHttp().getRequest().route?.path || request.url`, on observable completion call `metricsService.recordHttpRequest()` with duration. This interceptor should be applied globally via `APP_INTERCEPTORS` in `AppModule`. This replaces the optional metrics integration in the existing `LoggingInterceptor` -- instead, remove the `IMetricsService` injection from `LoggingInterceptor` and let it focus on logging only, while `PerformanceInterceptor` handles metrics exclusively. This separation of concerns is cleaner.

### Step 4 -- Structured Logging Enhancements

**4a. Enhance `apps/backend/src/logging/pino.config.ts`**: In the `customProps` function, add `userId` extraction: check `req.user?.sub` or `req.user?.id` (if the JWT is decoded by passport middleware before logging, otherwise this will be `undefined` for unauthenticated requests -- that is fine). Add `route` property: `req.route?.path || req.url`. Ensure the `customAttributeKeys` maps `responseTime` to `duration`.

**4b. Create `apps/backend/src/common/filters/error-tracking.filter.ts`** -- `ErrorTrackingFilter` implementing `ExceptionFilter`. Inject `Logger` (Pino) and `ConfigService`. In `catch(exception, host)`: (1) extract request from `host.switchToHttp().getRequest()`, get correlationId, userId, route, (2) log the error with full context including stack trace, (3) if Sentry is configured (`SENTRY_DSN` env var), call `Sentry.captureException(exception, { extra: { correlationId, userId, route, body: request.body }, tags: { route, method: request.method } })` -- install `@sentry/nestjs` package, initialize Sentry in `main.ts` with `Sentry.init({ dsn, environment, tracesSampleRate: 0.1 })`, (4) rethrow the exception so NestJS default filters still format the HTTP response. Register this filter globally via `APP_FILTER` in `AppModule` providers (but with lower priority than the default `HttpExceptionFilter` -- use `useClass` not `useExisting`).

### Step 5 -- Alerting System

Create `apps/backend/src/modules/monitoring/` with:

**5a. `monitoring.module.ts`** -- Import `TypeOrmModule.forFeature([ServiceStatus, IncidentLog, AlertRule, PerformanceBudget, WebVitalReport])`, `MetricsModule`, `HealthModule`, `EmailModule`, `ScheduleModule`. Register all services and controllers.

**5b. `services/alert-evaluator.service.ts`** -- Injectable service injecting `MetricsService`, `EmailService`, `ConfigService`, and TypeORM repositories. CRON `@Cron('*/60 * * * * *')` (every 60 seconds) method `evaluateAlerts(): Promise<void>`: (1) query all `AlertRule` where `isActive = true`, (2) for each rule, get the current metric value from Prometheus registry via `metricsService.getRegistry().getSingleMetric(rule.metric)`, (3) extract the relevant value (for counters, compute rate over `rule.duration` seconds by storing previous values in a Redis-backed rolling window keyed `alert-metric:${rule.id}` -- store `{ timestamp, value }` pairs, compute rate as `(currentValue - oldestValueInWindow) / windowSeconds`; for histograms, extract p99 from `histogram.get().values` filtering for quantile 0.99; for gauges, use current value), (4) evaluate `rule.condition` against `rule.threshold`, (5) if condition met and `lastTriggeredAt` is null or more than `cooldownMinutes` ago, trigger alert: update `lastTriggeredAt` and `triggerCount`, send notification via configured `channel`, (6) if channel is EMAIL, use `EmailService.sendMail()` with template `alert-notification` (create `apps/backend/src/modules/email/templates/alert-notification.hbs` -- include alert name, current value, threshold, timestamp, link to admin dashboard), if WEBHOOK, send HTTP POST to each recipient URL with JSON payload `{ alertName, metric, currentValue, threshold, condition, timestamp, severity }` using `fetch()`, if SLACK, post to Slack webhook URL with a formatted message block.

**5c. `services/alert-management.service.ts`** -- CRUD service for `AlertRule` entities. Method `seedDefaultRules(): Promise<void>` called on module init -- insert these rules if they don't already exist (check by name): (1) "High Error Rate" -- metric `api_errors_total`, condition `GT`, threshold `0.05` (5% of requests), duration 120s, channel EMAIL, priority P1, (2) "High P99 Latency" -- metric `http_request_duration_seconds`, condition `GT`, threshold 5.0, duration 180s, channel EMAIL, priority P2, (3) "Health Check Failure" -- metric `health_check_status` (custom -- see step 5d), condition `EQ`, threshold 0, duration 60s, channel EMAIL, priority P1, (4) "High Memory Usage" -- metric `nodejs_heap_used_bytes`, condition `GT`, threshold `500000000` (500MB), duration 300s, channel EMAIL, priority P3, (5) "Redis Connection Lost" -- metric `redis_health_status` (custom), condition `EQ`, threshold 0, duration 30s, channel EMAIL, priority P1. Default recipients: read from `ALERT_EMAIL_RECIPIENTS` env var (comma-separated).

**5d. Register custom health metrics**: In the health check CRON (create `services/health-monitor.service.ts` with `@Cron('*/30 * * * * *')`), run the detailed health check, for each service set a Gauge metric `health_check_status` with label `service` to 1 (UP) or 0 (DOWN/DEGRADED). This allows alert rules to reference health check status. Also record `health_check_latency_ms` Gauge per service. Also update `ServiceStatus` rows and compute rolling `uptimePercentage`: load last 90 days of status checks from metadata or a separate counter, calculate `(upChecks / totalChecks) * 100`.

### Step 6 -- Status Page and Incident Management

**6a. Create controller `apps/backend/src/modules/monitoring/controllers/status.controller.ts`**: Route prefix `status`. Public `GET /` -- return all `ServiceStatus` rows sorted by `serviceName`, plus any active incidents (status not RESOLVED) from `IncidentLog`. Public `GET /history` -- return `IncidentLog` entries from last 90 days, paginated. Public `GET /uptime` -- return per-service uptime percentages for last 90 days.

**6b. Create controller `controllers/incidents.controller.ts`**: Admin-only, route prefix `monitoring/incidents`. `POST /` -- create incident (body: `{ title, description, severity, servicesAffected }`), automatically add first timeline entry `{ timestamp: now, update: description }`. `PATCH /:id` -- update incident status, add timeline entry (body: `{ status, update }`). `PATCH /:id/resolve` -- set status to RESOLVED, `resolvedAt` to now, add "Resolved" timeline entry. `GET /` -- list all incidents, filter by status and severity. `GET /:id` -- get single incident with full timeline. `POST /:id/postmortem` -- set `postmortemUrl`.

**6c. Create controller `controllers/alerts.controller.ts`**: Admin-only, route prefix `monitoring/alerts`. CRUD for alert rules. `GET /active` -- list currently triggered alerts (rules where `lastTriggeredAt` within last `cooldownMinutes`). `POST /:id/test` -- fire a test alert notification for the given rule without updating `lastTriggeredAt`.

**6d. Create controller `controllers/performance.controller.ts`**: Route prefix `monitoring/performance`. Public `POST /web-vitals` -- receive Web Vitals reports from frontend (body: `{ route, metrics: Array<{ name, value, rating }>, userAgent, country, connectionType }`), save as `WebVitalReport` rows, update corresponding `PerformanceBudget.currentP75`/`currentP99`/`isCompliant`/`sampleCount` using a running percentile calculation. Admin-only `GET /budgets` -- list all performance budgets. Admin-only `POST /budgets` -- create/update budget. Admin-only `GET /vitals` -- query `WebVitalReport` aggregated by route and metric, with date range filter, return p75/p99 values and compliance status.

### Step 7 -- Deployment Verification

**7a. Create `apps/backend/src/modules/monitoring/services/deployment-verification.service.ts`**: Injectable service. Method `runSmokeTests(): Promise<{ passed: boolean; results: Array<{ test: string; passed: boolean; duration: number; error?: string }> }>`. Tests: (1) call `GET /health/ready` -- expect 200, (2) call `GET /api/v1/digital-products?limit=1` -- expect 200 with data array, (3) call `GET /metrics` -- expect 200 with Prometheus text, (4) check database migration status via `dataSource.query("SELECT * FROM typeorm_metadata")` or `dataSource.showMigrations()` -- verify no pending migrations, (5) call `RedisService.isHealthy()` directly. Return structured results. Method `onApplicationBootstrap()` (implement `OnApplicationBootstrap`): if `RUN_SMOKE_ON_DEPLOY` env var is `'true'`, run smoke tests 10 seconds after boot (use `setTimeout`), log results, if any test fails and `AUTO_ROLLBACK_ENABLED` is `'true'`, log critical error and call `process.exit(1)` (which will trigger Docker/K8s restart policy or container orchestrator rollback).

**7b. Create deployment health check endpoint**: `POST /monitoring/deploy/verify` (admin-only) -- manually trigger smoke tests and return results. This can be called from CircleCI post-deploy step.

### Step 8 -- Frontend Status Page

Create `apps/frontend/app/[locale]/status/page.tsx` as a public page (no auth required). Fetch `GET /api/v1/status` on load. Display: (1) Overall system status banner at top -- green "All Systems Operational" if all services UP, yellow "Degraded Performance" if any DEGRADED, red "System Outage" if any DOWN. (2) Service grid: for each service, a card showing name, status indicator (green/yellow/red circle with pulse animation via Framer Motion for DOWN), latency in ms, 90-day uptime percentage with a tiny Recharts `AreaChart` sparkline showing uptime over time (fetch from `/status/uptime`). (3) Active incidents section: if any, display as cards with severity badge, title, latest timeline entry, "Last updated X minutes ago". (4) Incident history: collapsible section showing past 90 days of resolved incidents as a timeline (vertical line with dots and cards). Style the page to be clean and professional -- this is public-facing. Add `<meta>` tags for SEO. Add auto-refresh every 60 seconds using `useQuery` with `refetchInterval: 60000`.

### Step 9 -- Frontend Web Vitals Reporting

**9a. Create `apps/frontend/lib/web-vitals.ts`**: Import `web-vitals` package (`npm install web-vitals`). Function `reportWebVitals()`: call `onLCP`, `onFID`, `onCLS`, `onFCP`, `onTTFB`, `onINP` from `web-vitals`. Batch metrics: collect them in an in-memory array, debounce for 5 seconds, then POST to `POST /api/v1/monitoring/performance/web-vitals` with the batch payload `{ route: window.location.pathname, metrics: [...], userAgent: navigator.userAgent, connectionType: navigator.connection?.effectiveType }`. Only report in production (`process.env.NODE_ENV === 'production'`).

**9b. Initialize in layout**: In `apps/frontend/app/[locale]/layout.tsx`, call `reportWebVitals()` in a `useEffect` on mount (client-side only).

### Step 10 -- Admin Monitoring Dashboard

Add to `apps/dashboard/` (the admin dashboard app): a monitoring section with:

**10a. Real-time metrics page**: Fetch `/metrics` endpoint, parse Prometheus text format (use `prom-client` parser or a simple regex parser), display key metrics as Recharts `LineChart` components with 5-minute auto-refresh: request rate (requests/sec), error rate (%), p50/p95/p99 latency, active connections, memory usage. Each chart shows last 1 hour of data (store data points in React state, append on each fetch, trim to 60 points).

**10b. Active alerts panel**: Fetch `GET /api/v1/monitoring/alerts/active`. Display as a sidebar or top banner with severity-colored badges. Click to see alert details and acknowledge.

**10c. Incident manager page**: Full CRUD UI for incidents. Create incident form (title, description, severity dropdown, services affected multi-select from service list). Incident detail page showing timeline as a vertical stepper, "Add Update" form, "Resolve" button. List view with filters.

**10d. Performance budgets page**: Table showing all routes with their budget values and current p75/p99. Color code: green if compliant, red if exceeded. Click a route to see a Recharts `LineChart` of its Web Vitals over time (query `/monitoring/performance/vitals?route=X`).

**10e. Deployment history**: If a `deployment_logs` table or CI webhook integration exists, show recent deploys with smoke test results. Otherwise, add a simple `GET /monitoring/deploy/history` endpoint that returns recent smoke test results stored in Redis with 7-day TTL.

### Step 11 -- Register Module and Wire Everything

Add `MonitoringModule` to `imports` in `apps/backend/src/app.module.ts`. Register `PerformanceInterceptor` as a global interceptor via `APP_INTERCEPTORS` provider in `AppModule`. Register `ErrorTrackingFilter` as a global filter via `APP_FILTER`. Add environment variables to `.env.example`: `SENTRY_DSN`, `ALERT_EMAIL_RECIPIENTS=admin@drkatangablog.com`, `RUN_SMOKE_ON_DEPLOY=false`, `AUTO_ROLLBACK_ENABLED=false`, `SLACK_WEBHOOK_URL` (optional). Generate TypeORM migration for the five new tables.
